{"cells":[{"metadata":{},"cell_type":"markdown","source":"https://github.com/google-research/bert"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string\nimport re\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/pretrained-bert-including-scripts/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/vocab.txt\n/kaggle/input/pretrained-bert-including-scripts/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n/kaggle/input/pretrained-bert-including-scripts/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt.index\n/kaggle/input/pretrained-bert-including-scripts/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt.meta\n/kaggle/input/pretrained-bert-including-scripts/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/vocab.txt\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/bert_model.ckpt.index\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/bert_model.ckpt.meta\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/bert_config.json\n/kaggle/input/pretrained-bert-including-scripts/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/vocab.txt\n/kaggle/input/pretrained-bert-including-scripts/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n/kaggle/input/pretrained-bert-including-scripts/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/bert_model.ckpt.index\n/kaggle/input/pretrained-bert-including-scripts/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta\n/kaggle/input/pretrained-bert-including-scripts/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/bert_config.json\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/vocab.txt\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/bert_model.ckpt.index\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/bert_model.ckpt.meta\n/kaggle/input/pretrained-bert-including-scripts/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/bert_config.json\n/kaggle/input/pretrained-bert-including-scripts/cased_L-24_H-1024_A-16/cased_L-24_H-1024_A-16/vocab.txt\n/kaggle/input/pretrained-bert-including-scripts/cased_L-24_H-1024_A-16/cased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001\n/kaggle/input/pretrained-bert-including-scripts/cased_L-24_H-1024_A-16/cased_L-24_H-1024_A-16/bert_model.ckpt.index\n/kaggle/input/pretrained-bert-including-scripts/cased_L-24_H-1024_A-16/cased_L-24_H-1024_A-16/bert_model.ckpt.meta\n/kaggle/input/pretrained-bert-including-scripts/cased_L-24_H-1024_A-16/cased_L-24_H-1024_A-16/bert_config.json\n/kaggle/input/pretrained-bert-including-scripts/cased_L-12_H-768_A-12/cased_L-12_H-768_A-12/vocab.txt\n/kaggle/input/pretrained-bert-including-scripts/cased_L-12_H-768_A-12/cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n/kaggle/input/pretrained-bert-including-scripts/cased_L-12_H-768_A-12/cased_L-12_H-768_A-12/bert_model.ckpt.index\n/kaggle/input/pretrained-bert-including-scripts/cased_L-12_H-768_A-12/cased_L-12_H-768_A-12/bert_model.ckpt.meta\n/kaggle/input/pretrained-bert-including-scripts/cased_L-12_H-768_A-12/cased_L-12_H-768_A-12/bert_config.json\n/kaggle/input/bertsrc/modeling.py\n/kaggle/input/bertsrc/requirements.txt\n/kaggle/input/bertsrc/__init__.py\n/kaggle/input/bertsrc/multilingual.md\n/kaggle/input/bertsrc/create_pretraining_data.py\n/kaggle/input/bertsrc/optimization_test.py\n/kaggle/input/bertsrc/run_classifier.py\n/kaggle/input/bertsrc/run_squad.py\n/kaggle/input/bertsrc/run_classifier_with_tfhub.py\n/kaggle/input/bertsrc/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n/kaggle/input/bertsrc/tokenization_test.py\n/kaggle/input/bertsrc/run_pretraining.py\n/kaggle/input/bertsrc/tokenization.py\n/kaggle/input/bertsrc/sample_text.txt\n/kaggle/input/bertsrc/CONTRIBUTING.md\n/kaggle/input/bertsrc/extract_features.py\n/kaggle/input/bertsrc/LICENSE\n/kaggle/input/bertsrc/modeling_test.py\n/kaggle/input/bertsrc/optimization.py\n/kaggle/input/bertsrc/README.md\n/kaggle/input/hebrew-corpus-oscar/he_dedup.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# ! pip install -U tokenizers","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install tensorflow==1.15","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting tensorflow==1.15\n  Downloading tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3 MB)\n\u001b[K     |████████████████████████████████| 412.3 MB 28 kB/s  eta 0:00:011|████▋                           | 59.9 MB 50.3 MB/s eta 0:00:08     |█████████████████████████████   | 373.9 MB 47.2 MB/s eta 0:00:01     |█████████████████████████████   | 374.2 MB 47.2 MB/s eta 0:00:01     |██████████████████████████████▏ | 388.7 MB 47.2 MB/s eta 0:00:01     |██████████████████████████████▎ | 389.8 MB 7.7 MB/s eta 0:00:03     |██████████████████████████████▍ | 392.1 MB 7.7 MB/s eta 0:00:03     |██████████████████████████████▌ | 393.2 MB 7.7 MB/s eta 0:00:03     |███████████████████████████████▋| 407.7 MB 7.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.27.2)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.9.0)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.11.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (3.2.0)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.18.2)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.1.0)\nRequirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.0.8)\nRequirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.2.2)\nCollecting tensorflow-estimator==1.15.1\n  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n\u001b[K     |████████████████████████████████| 503 kB 38.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.2.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.14.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.34.2)\nCollecting tensorboard<1.16.0,>=1.15.0\n  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 39.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.8.1)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (3.11.3)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.1.0)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (46.1.3.post20200330)\nInstalling collected packages: tensorflow-estimator, tensorboard, tensorflow\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.1.0\n    Uninstalling tensorflow-estimator-2.1.0:\n      Successfully uninstalled tensorflow-estimator-2.1.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.1.1\n    Uninstalling tensorboard-2.1.1:\n      Successfully uninstalled tensorboard-2.1.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.1.0\n    Uninstalling tensorflow-2.1.0:\n      Successfully uninstalled tensorflow-2.1.0\nSuccessfully installed tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Path = '/kaggle/input/hebrew-corpus-oscar/he_dedup.txt'","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cleaning text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #remove punctutaion\n# ' '.join(word.strip(string.punctuation) for word in rrr.split())\n# #remove chinese characters in tokenizers\n# #remove non hebrew character\n# [w for w in rrr if not re.match(r'[A-Za-z]+', w, re.I)]\n# #remove numbers\n\n\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample - 1000 rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(Path) as myfile:\n    head = [next(myfile) for x in range(1000)]\n# print(head)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Write sample to file txt**"},{"metadata":{"trusted":true},"cell_type":"code","source":"outF = open(\"mysample.txt\", \"w\")\nfor line in head:\n  # write line to output file\n  outF.write(line)\n  outF.write(\"\\n\")\noutF.close()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tokenizers","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bwpt = tokenizers.BertWordPieceTokenizer(\n    vocab_file=None,\n    add_special_tokens=True,\n    unk_token='[UNK]',\n    sep_token='[SEP]',\n    cls_token='[CLS]',\n    clean_text=True,\n    handle_chinese_chars=False,\n    strip_accents=True,\n    lowercase=True,\n    wordpieces_prefix='##'\n)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split original file to mini files"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !split -l 15000 /kaggle/input/hebrew-corpus-oscar/he_dedup.txt hebrew_\n# import glob\n# file_list = glob.glob(\"..../hebrew_*\")","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"bwpt.train(\n#     files=[\"../input/hindi-oscar-corpus/hi_dedup_1000.txt\"],\n    files = [\"../working/mysample.txt\"],\n    vocab_size=30000,\n    min_frequency=3,\n    limit_alphabet=1000,\n    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[MASK]', '[SEP]']\n)","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Save vocab**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bwpt.save(\"../working/\", \"hebrew\")","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"['../working/hebrew-vocab.txt']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"let's check it out"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = bwpt.encode(\"היי, מה המצב?\")\nprint(encoded)","execution_count":13,"outputs":[{"output_type":"stream","text":"Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd ../input/bertsrc/","execution_count":14,"outputs":[{"output_type":"stream","text":"/kaggle/input/bertsrc\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":15,"outputs":[{"output_type":"stream","text":"CONTRIBUTING.md\t\t    optimization_test.py\r\nLICENSE\t\t\t    predicting_movie_reviews_with_bert_on_tf_hub.ipynb\r\nREADME.md\t\t    requirements.txt\r\n__init__.py\t\t    run_classifier.py\r\ncreate_pretraining_data.py  run_classifier_with_tfhub.py\r\nextract_features.py\t    run_pretraining.py\r\nmodeling.py\t\t    run_squad.py\r\nmodeling_test.py\t    sample_text.txt\r\nmultilingual.md\t\t    tokenization.py\r\noptimization.py\t\t    tokenization_test.py\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Create pretraining data"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!python create_pretraining_data.py \\\n    --input_file=/kaggle/working/mysample.txt \\\n    --output_file=/kaggle/working/tf_examples.tfrecord \\\n    --vocab_file=/kaggle/working/hebrew-vocab.txt \\\n    --do_lower_case=True \\\n    --max_seq_length=128 \\\n    --max_predictions_per_seq=20 \\\n    --masked_lm_prob=0.15 \\\n    --random_seed=42 \\\n    --dupe_factor=5","execution_count":16,"outputs":[{"output_type":"stream","text":"W0421 11:32:47.797061 140538879149440 module_wrapper.py:139] From create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n\nW0421 11:32:47.797339 140538879149440 module_wrapper.py:139] From create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n\nW0421 11:32:47.797619 140538879149440 module_wrapper.py:139] From /kaggle/input/bertsrc/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n\nW0421 11:32:47.839276 140538879149440 module_wrapper.py:139] From create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n\nW0421 11:32:47.841992 140538879149440 module_wrapper.py:139] From create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n\nI0421 11:32:47.842214 140538879149440 create_pretraining_data.py:446] *** Reading from input files ***\nI0421 11:32:47.842410 140538879149440 create_pretraining_data.py:448]   /kaggle/working/mysample.txt\nI0421 11:32:54.942204 140538879149440 create_pretraining_data.py:457] *** Writing to output files ***\nI0421 11:32:54.942485 140538879149440 create_pretraining_data.py:459]   /kaggle/working/tf_examples.tfrecord\nW0421 11:32:54.942806 140538879149440 module_wrapper.py:139] From create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n\nI0421 11:32:54.943882 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.944108 140538879149440 create_pretraining_data.py:151] tokens: [CLS] [MASK] המע ##מד מורכב מב ##סיס [MASK] [MASK] כפי שניתן לראות בתמונה המרכז ##ית או זוג רג ##ליים [MASK] ##יע ##ות בתמונה הי ##מנית . [SEP] אחת אית ##ות המדובר ##ות של השנים האחרונות היא עיבוד לספר ##ה של הס ##ופר ##ת הק ##נדי ##ת המוע ##רכת [MASK] ##רט [MASK] [MASK] ##וד , שני ##גשה לד ##ון בס ##וגיות [MASK] ##ות בפ ##מיני ##זם . . . המשך [SEP]\nI0421 11:32:54.944353 140538879149440 create_pretraining_data.py:161] input_ids: 2 3 1343 888 3672 810 3827 3 3 1018 1922 1788 6696 2722 625 734 2843 1690 2159 3 774 621 6696 682 6147 18 4 1183 4162 621 5114 621 627 2402 2202 857 3108 6977 362 627 723 1809 355 715 4282 355 3161 1107 3 1159 3 3 630 16 1062 3817 864 649 802 6585 3 621 777 5352 5045 18 18 18 1505 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nI0421 11:32:54.944564 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nI0421 11:32:54.944767 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nI0421 11:32:54.944926 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 1 7 8 13 19 28 48 50 51 60 0 0 0 0 0 0 0 0 0 0\nI0421 11:32:54.945090 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 2598 5661 2799 2722 3481 4498 4223 108 1356 2629 0 0 0 0 0 0 0 0 0 0\nI0421 11:32:54.945304 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nI0421 11:32:54.945455 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.946118 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.946365 140538879149440 create_pretraining_data.py:151] tokens: [CLS] חג ' ג ' איר ##ופה ריאלי – נדל \" [MASK] ובינוי – נדל \" ן ובינוי עוד חגג , דרך מנחם [MASK] ##1 [MASK] , תל ##ימוש 03 - 6081 ##73 ##3 03 - 6081 ##73 [MASK] [SEP] כמו שאר רבני מרוקו , גם רבני מכנאס עסק ##ו בעיקר בספר [MASK] ההלכה . כמעט כולם ##דש ##ים [MASK] ##ופות של שו \" ת ושל הלכות [MASK] הרי הן התש ##ובות הכתובות הני ##ת ##נות לש ##אלות המופ ##נות אליהם בעניינים [MASK] ##יים . התש ##ובות הללו מהו [MASK] לעתים [MASK] ##ות מקור [MASK] [MASK] יס ##ולא ##ו בפ ##ז לבי ##רו ##ר [MASK] החברה היהודית באותה תקופה . הדבר הומ ##ח ##ש [MASK] ביצ ##ירותיהם של רבני מכנאס לד ##ורותיהם . [SEP]\nI0421 11:32:54.946594 140538879149440 create_pretraining_data.py:161] input_ids: 2 2215 11 110 11 2655 1459 1643 203 1441 6 3 1730 203 1441 6 123 1730 1141 7267 16 1262 4732 3 397 3 16 1033 1876 1043 17 7186 2545 400 1043 17 7186 2545 3 4 1155 3581 3279 1375 16 743 3279 1483 1377 352 1831 2179 3 3933 18 2205 2672 1283 622 3 6512 627 3120 6 134 2483 4587 3 1120 1634 2058 1181 5696 1635 355 660 728 1775 3481 660 6755 6488 3 781 18 2058 1181 3599 3915 3 3310 3 621 2778 3 3 2014 6327 352 777 358 1264 636 372 3 2065 2976 3208 3129 18 3543 4871 378 375 3 5410 6675 627 3279 1483 864 5590 18 4 0 0 0 0\nI0421 11:32:54.946789 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\nI0421 11:32:54.946981 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\nI0421 11:32:54.947133 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 11 23 25 28 38 52 57 59 61 67 82 89 90 91 94 95 103 104 114 0\nI0421 11:32:54.947279 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 123 3260 6266 1316 393 621 3564 945 627 16 1531 621 3310 2691 621 938 372 3806 2411 0\nI0421 11:32:54.947426 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0421 11:32:54.947556 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.948174 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.948456 140538879149440 create_pretraining_data.py:151] tokens: [CLS] יש לע ##יין בגוף כוונת ההע ##רה , דה ##לא רק [MASK] מוע ##טים בה ##גה ##ות אלו [MASK] [MASK] [MASK] [MASK] ק לפרש דברי התניא , ובו ##ודאי יש דברים בג ##ו שמצא לנ [MASK] לשלול הפירוש שקאי על הנפש בצאתה מן הגוף , ולכאורה מה [MASK] אם היינו מפר [MASK] כן [MASK] [MASK] שא ##כן פיר [MASK] [MASK] כנ \" ל [SEP] קיימת תנייה המט [MASK] [MASK] המבוטח חובת זהירות - משמעות התנייה תה ##א שנ ##דרשת רשלנות [MASK] במ ##וב ##נה הנ \" ל על [MASK] לקבוע שח ##ובת הזהירות הופ ##רה תורת האשם החוזי התורם מנוגד ##ת , למי [MASK] הב ##נתי , לג ##ישה המקו ##בלת בדי [MASK] הביטוח לפיה מג ##ינה פוליסת הביטוח על המבוטח גם מפני רשלנותו - הוא [SEP]\nI0421 11:32:54.948726 140538879149440 create_pretraining_data.py:161] input_ids: 2 771 759 1303 5455 7350 6595 710 16 1794 703 978 3 2226 847 718 1321 621 1209 3 3 3 3 131 2805 1918 4421 16 4883 2908 771 1738 832 352 5404 1374 3 5337 6574 4657 639 5387 5829 1059 2070 16 6746 702 3 862 4453 2674 3 1113 3 3 814 850 1947 3 3 1405 6 120 4 2460 6116 2549 3 3 964 2342 2586 17 1571 6430 2134 374 869 6752 1319 3 647 645 638 761 6 120 639 3 2448 1253 1240 4660 2012 710 4279 5169 6497 5201 7352 355 16 5214 3 758 4444 16 977 1082 1966 5057 1852 3 967 2074 841 1649 3111 967 639 964 743 4009 5600 17 737 4\nI0421 11:32:54.948958 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.949169 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.949332 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 12 19 20 21 22 26 36 48 52 54 55 59 60 68 69 81 89 104 113 0\nI0421 11:32:54.949500 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 3702 1515 625 119 6 4421 1610 7179 799 12 1018 1092 1900 812 639 1845 1202 1883 628 0\nI0421 11:32:54.949653 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0421 11:32:54.949774 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.950385 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.950603 140538879149440 create_pretraining_data.py:151] tokens: [CLS] ##למה אלקטרוני [MASK] המח ##וברת לט ##לסקופ ומח [MASK] ##פה את [MASK] האדם בתור הגל ##אי . שב ##ב [MASK] ##למה קול ##ט את אור הכוכבים ( קרינה אלקטרומגנטית ) , והופ [MASK] אותו לא ##ותו [MASK] חשמל ##יים אשר [MASK] ##ובדים על ידי מחשב לת ##מונה אסטרונומי ##ופן אותה ניתן לנת ##ח לאחר מכן [MASK] ראה גם מאמר [MASK] תמונות ccd / [SEP] אג ##רו ##ל אבוקדו . הקר ##פ ##צ ' ו היה טעים , אם כי קצת רג ##יל [MASK] וחיפ ##שנו [MASK] אוכל מיוחד . הא ##גר ##ולים , שלושה במספר רוב היו [MASK] ממש . [MASK] בדרך כלל לא [MASK] ##ב אבוקדו [MASK] אבל כאן , זה היה ממש מש ##וב ##ח . ר ##סק האב ##וקדו הח ##ם בתוך הא [SEP]\nI0421 11:32:54.950832 140538879149440 create_pretraining_data.py:161] input_ids: 2 3822 4130 3 960 5019 1941 1532 3947 3 751 637 3 1710 4540 5342 689 18 778 369 3 3822 3413 377 637 1024 1295 12 3796 7622 13 16 4442 3 1016 648 1123 3 6976 781 910 3 5149 639 853 1673 912 1290 5663 1312 1367 1009 4926 378 991 2989 3 2508 743 5266 3 2363 5814 19 4 1629 636 361 4572 18 6576 373 370 11 113 767 4200 16 862 677 2690 1690 643 3 3283 6214 3 1465 2018 18 651 1013 1273 16 3286 2621 2505 1017 3 1076 18 3 1541 1259 648 3 369 4572 3 1368 1781 16 735 767 1076 652 645 378 18 132 733 1971 3900 680 371 1511 651 4\nI0421 11:32:54.951052 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.951251 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.951408 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 3 9 12 20 33 37 41 49 56 60 67 83 86 96 98 101 105 108 113 0\nI0421 11:32:54.951560 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 355 650 2127 1388 394 355 724 355 18 2622 361 16 4687 16 6614 1044 4542 18 767 0\nI0421 11:32:54.951689 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0421 11:32:54.951825 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.952439 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.952652 140538879149440 create_pretraining_data.py:151] tokens: [CLS] [MASK] חבר שלו ' , כך הוא סיפר לי [MASK] לא הצליח למכור את הרכב המש ##ומש שלו זמן ארוך , וב ##לח ##יצ ##ת כפתור ומש ##לוח הודעה פרס ##ומית על הרכב המש ##ומש הביאה במהרה , [MASK] חוק מ 5 [MASK] פניו ##ת של מתע ##ניינים בר ##כי ##שת הרכב . [SEP] גם אני תמיד חשש [MASK] [MASK] [MASK] ##עון , אבל אני חייבת [MASK] שזה פחות [MASK] ##א ממה [MASK] ##שב ##תי . הה ##בדל בטעם בין דובדבנים [MASK] למש ##ומרים הוא הב ##דל של שמי ##ים [MASK] [MASK] ולכן באמת שווה להש ##קי ##ע עוד כמה דקות בגיל ##עון . [SEP]\nI0421 11:32:54.952860 140538879149440 create_pretraining_data.py:161] input_ids: 2 3 900 1160 11 16 927 737 4071 885 3 648 5364 6463 637 998 705 6467 1160 1489 2453 16 754 1650 1049 355 3759 1933 1893 3292 1727 3516 639 998 705 6467 4581 7004 16 3 1074 122 25 3 4673 355 627 3553 1997 770 784 763 998 18 4 743 1044 2698 4646 3 3 3 3811 16 1368 1044 5514 3 2510 2232 3 374 4216 3 889 685 18 721 6193 5908 929 4176 3 1426 2409 737 758 979 627 1870 622 3 3 1780 3912 2870 2153 1560 354 1141 1940 2010 2783 3811 18 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nI0421 11:32:54.953054 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nI0421 11:32:54.953257 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nI0421 11:32:54.953419 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 1 10 16 39 40 43 59 60 61 67 70 73 82 91 92 95 0 0 0 0\nI0421 11:32:54.953582 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 11 16 705 1132 2232 4353 685 702 1872 2347 4229 1253 4901 722 981 2870 0 0 0 0\nI0421 11:32:54.953736 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0\nI0421 11:32:54.953866 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.954492 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.954719 140538879149440 create_pretraining_data.py:151] tokens: [CLS] [MASK] דקות במק ##פיא . אפשר להכין את הבצק ערב לפני ולה ##ניח [MASK] ##לה במקרר . [MASK] [MASK] ##ור , מניחים את הבצק [MASK] גבי משטח עבודה מקו [MASK] היטב [MASK] ##דים לע [MASK] בקוטר של כ - 30 ס \" [MASK] ( עי ##גול [MASK] גדול מעט מקו ##טר אסטרונומיות ##בנית ) . גם כאן [MASK] להקפיד על עיבוד מינימלי ורי ##דוד מהיר של הבצק , על מנת שלא ית ##ח ##מם [MASK] [MASK] קשה לעבוד ##ה . [SEP] תוך כדי יכולת תנועה חופשית לחלוטין , [MASK] ##מום מאמץ . ארגונו ##מיה המאפשרת הת ##מ ##מש [MASK] [MASK] ##ה יותר בין המשתמש לז ##וג הצג [MASK] שע ##ומד בפניו וכך [MASK] ##נות מפר ##יסה בתצ ##ורה המות ##א ##מות באופן אישי ישירות אליו . [SEP]\nI0421 11:32:54.954983 140538879149440 create_pretraining_data.py:161] input_ids: 2 3 2010 1171 6525 18 1139 7334 637 2097 2128 1219 1153 4373 3 673 4050 18 3 3 623 16 3691 637 2097 3 1737 3887 1462 1749 3 2411 3 746 759 3 7246 627 119 17 1590 125 6 3 12 2230 4297 3 1245 2169 1749 769 4709 6191 13 18 743 1781 3 7433 639 3108 5761 4877 1499 3213 627 2097 16 639 1202 938 2218 378 1647 3 3 2689 3561 362 18 4 1254 954 4663 2458 5750 3317 16 3 5211 4088 18 3695 1446 4055 658 356 699 3 3 362 1075 929 7408 1799 716 2299 3 902 1829 7172 3003 3 660 2674 4426 4539 727 3856 374 792 1427 3295 5403 3248 18 4\nI0421 11:32:54.955278 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.955527 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.955696 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 1 14 18 19 25 30 32 35 43 47 52 58 75 76 89 99 100 108 113 0\nI0421 11:32:54.955854 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 1590 4923 991 6578 639 1696 6631 6960 122 7246 658 2629 3746 634 6376 1189 4949 622 2726 0\nI0421 11:32:54.956023 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0421 11:32:54.956170 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.956753 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.956986 140538879149440 create_pretraining_data.py:151] tokens: [CLS] אז [MASK] גם אתה קיבל ##ת הודעות פרסומת [MASK] רצויות יכול להיות שמ [MASK] לך יחסית בלי הרבה מאמץ , פי ##צוי 4 עד 1 , 000 ₪ לכל הודעה באמצעות פניה לבית המשפט לת ##ביעות [MASK] ואולי אפילו [MASK] מקום וק ##מה לך הז ##כות [MASK] להגיש נגד שול ##חי [MASK] ##ומות הבלתי רצויות גם תביעה ייצוגית [MASK] ##לי ##ונים . [SEP] שאינה מהרהרת – אמ \" ש בת ##ניא [MASK] באתר וע ##סק התורה ומצ ##ות והתפלה הוא ג \" כ ענין מסירת נפש ממש כמו בצאתה מן [MASK] במלא [MASK] שבע ##ים שנה שאינה מהרהרת בצרכי הגוף אלא מחשבתה מיוחדת [MASK] באותיות לצפון והתפלה שה ##ן דבר ה ' [MASK] ##שב ##תו ית ' והי ##ו לא ##חדים ממש שזה ##ו כל עסק [SEP]\nI0421 11:32:54.957222 140538879149440 create_pretraining_data.py:161] input_ids: 2 957 3 743 3173 3037 355 4059 4096 3 7328 1263 1453 772 3 2853 2456 1853 1713 4088 16 823 2034 24 918 21 16 1735 211 1100 3292 1510 6059 1622 905 912 6337 3 4496 2595 3 1165 1318 691 2853 930 884 3 2325 1281 6091 937 3 2280 2973 7328 743 2632 4755 3 650 1012 18 4 2801 4142 203 1072 6 133 762 2723 3 1833 795 733 1424 3548 621 3040 737 110 6 119 4246 6553 2680 1076 1155 5829 1059 3 3877 3 2431 622 1381 2801 4142 5474 2070 1056 5655 4721 3 3209 6802 3040 683 360 1130 112 11 3 889 690 2218 11 5219 352 648 1960 1076 2510 352 694 1377 4\nI0421 11:32:54.957446 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.957653 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.957803 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 2 9 11 14 19 23 37 40 47 52 53 59 72 73 91 93 104 106 113 0\nI0421 11:32:54.957963 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 862 648 1263 2245 4088 627 7024 771 16 7020 2280 4400 1047 6 2070 355 7618 1424 3947 0\nI0421 11:32:54.958120 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0421 11:32:54.958247 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.958819 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.959038 140538879149440 create_pretraining_data.py:151] tokens: [CLS] בית המשפט המחוזי : אחריות [MASK] של המשיב מס ' [MASK] א [MASK] ' רשלנות ' בשמירת התכשיטים [MASK] , לשיטת הרו ##ב [MASK] המשפט [MASK] , המשיב מס ' 2 לא התרשל בשמירת התכשיטים עובר [MASK] ##ניבת [MASK] . אין אני מסכים עם עמדה זו ומק ##ובלת עלי עמדת בית משפט השלום וע ##מדת המי ##עו ##ט בבית המשפט המחוזי בירדוגו לפיה [SEP] [MASK] [MASK] ברקת במכבים , פתוח לאנ ##שים בעלי עניין באסטרונומיה הרו ##צים לראות ול ##למוד על השמיים והח ##לל באופן מדעי , עם הרבה הנאה . מצפה הכוכבים [MASK] הגדול [MASK] באזור [MASK] והוא בעל ציוד ייחודי . המצפה קשור לגו ##פי [MASK] בינלאומי ##ים ומ ##פעיל מגוון פרו [MASK] ייחודי ##ם [MASK] גם [MASK] מחקר בשל ##יטה מרח [MASK] . [SEP]\nI0421 11:32:54.959259 140538879149440 create_pretraining_data.py:161] input_ids: 2 766 905 1995 30 5297 3 627 3535 706 11 3 108 3 11 1319 11 6566 2571 3 16 4505 1542 369 3 905 3 16 3535 706 11 22 648 3508 6566 2571 2601 3 6322 3 18 867 1044 3919 765 2807 837 2584 2731 3492 4083 766 949 2057 795 2786 3855 1284 377 1229 905 1995 1786 2074 4 3 3 1907 4403 16 7194 6379 799 1574 1946 3531 1542 2140 1788 791 4306 639 3506 2576 1146 1427 5534 16 765 1713 4535 18 1986 1295 3 1774 3 2288 3 1896 1714 3035 5707 18 4068 7425 6900 697 3 7513 622 731 1678 4586 1550 3 5707 371 3 743 3 4226 1834 2168 5996 3 18 4\nI0421 11:32:54.959477 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.959702 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.959850 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 6 11 13 19 24 26 37 39 62 65 66 94 96 98 108 115 118 120 125 0\nI0421 11:32:54.960005 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 352 22 18 1839 1229 1995 977 371 16 1986 1295 737 7337 16 4708 4428 1488 2313 675 0\nI0421 11:32:54.960150 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0421 11:32:54.960270 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.960820 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.961051 140538879149440 create_pretraining_data.py:151] tokens: [CLS] [MASK] מיום [MASK] . 8 . 2018 גול ##לו העותרים שור ##ה [MASK] מקרים המוכיחים , לפיכך , כי ניתנה אפשרות לבעלי מבנים להגיש בקשות להיתר ##י בנייה , על אף שהגישו בש ##עתו לבית המשפט עתירה שנ ##דחתה , וכי [MASK] ##תם המקרים הסכ ##ימה [MASK] לעכב את ביצוע צו [MASK] . [SEP] בעת סילוק תביעתו [MASK] מבוטח יש לבדוק האם הופ ##ר התנאי הנוגע לחובות המיגון [MASK] הח ##כתב עליו המבטחת . לעניין זה [MASK] ##בח ##נו השאלות האם הות ##ק ##נו [MASK] המיגון כפי שנ [MASK] ##פיה על ידי המבטחת והא ##ם היה צורך להפעיל [MASK] בנסיבות המקרה הספציפי בכפוף לבחינת השאלות האם הרכב היה ##נג או בעת חניה וזאת כ ##קבוע בחוזה הביטוח שבין המבטחת [MASK] [MASK] [SEP]\nI0421 11:32:54.961300 140538879149440 create_pretraining_data.py:161] input_ids: 2 3 1637 3 18 28 18 1292 3740 659 984 4268 362 3 2597 7257 16 2322 16 677 4026 1728 6657 2599 2325 4580 5144 359 2439 16 639 1098 7508 708 1754 1622 905 2129 869 6134 16 1437 3 1089 2790 2069 3449 3 4532 637 1908 966 3 18 4 1530 7496 3648 3 2351 771 5368 1478 2012 372 5196 4028 6642 2148 3 680 1332 1421 1291 18 3559 735 3 1387 629 6424 1478 3070 367 629 3 2148 1018 869 3 3459 639 853 1291 4440 371 767 2233 2051 3 2477 1676 4744 6838 6656 6424 1478 998 767 1178 734 1530 4896 1848 119 6638 3992 967 5407 1291 3 3 4 0 0 0 0 0\nI0421 11:32:54.961524 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\nI0421 11:32:54.961719 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\nI0421 11:32:54.961882 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 1 3 13 17 42 47 52 58 69 71 77 85 89 90 99 109 120 121 0 0\nI0421 11:32:54.962031 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 3694 1536 627 5744 5273 1287 1331 627 1838 812 3079 1923 2076 352 371 1716 3898 18 0 0\nI0421 11:32:54.962200 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0\nI0421 11:32:54.962360 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.962961 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.963229 140538879149440 create_pretraining_data.py:151] tokens: [CLS] ##נד ##ון בע ##ובדות המקרה . [MASK] [MASK] [MASK] ##מור מדובר בש ##אלות עק ##רו ##ניות החלטנו להידרש לס ##וגיה העקרונית . לצורך זה הנח ##נו [MASK] עובד ##ות המקרה הן [MASK] בשעה שאשת המבוטח עסקה [MASK] ##מס ##ת עצ ##יצ ##ים [MASK] ##דר ##כה אל תוך הרכב , מאח ##ור , שעה שמ ##פתחות הרכב היו בתוך מתג ההנעה וכאשר הפ [MASK] המבוטחת את גבה לרכב - נכנס מא ##ן דה [SEP] יהודי מרוקו נהנו גם מא [MASK] [MASK] ##מיה משפטית ניכרת ##יפה קהילה היה בית דין [MASK] . [MASK] הדין טיפ ##ל בעניינים שבין העותרים היהודים על פי ההלכה ועל [MASK] [MASK] [MASK] . הפסיקה של בית הדין בוצ ##עה בידי הנגיד , אשר לרשות ##ו עמד בית כל ##א קטן במלא ##ח . [SEP]\nI0421 11:32:54.963470 140538879149440 create_pretraining_data.py:161] input_ids: 2 1204 649 679 4397 1676 18 3 3 3 1144 1599 708 1775 1864 636 1179 5677 7504 1163 4489 4783 18 1461 735 6685 629 3 3786 621 1676 1634 3 3216 6778 964 7064 3 1213 355 1032 1049 622 3 798 811 800 1254 998 16 3205 623 16 3126 772 2089 998 1017 1511 4524 5325 5780 713 3 6888 637 4853 2493 17 2681 692 360 1794 4 1910 1375 5610 743 692 3 3 1446 3613 3329 3820 6077 767 766 801 3 18 3 922 1939 361 6488 5407 984 1777 639 823 3933 1366 3 3 3 18 4484 627 766 922 5904 720 1985 5722 16 910 3646 352 1549 766 694 374 2452 3877 378 18 4\nI0421 11:32:54.963679 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.963874 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.964034 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 7 8 9 27 32 37 43 51 63 79 80 84 89 91 103 104 105 111 114 0\nI0421 11:32:54.964185 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 3064 4872 3267 677 677 3540 2760 623 2164 881 1695 1100 6399 766 823 1615 660 5904 5722 0\nI0421 11:32:54.964373 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0421 11:32:54.964501 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.965093 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.965331 140538879149440 create_pretraining_data.py:151] tokens: [CLS] הכפר וכי [MASK] נקבע בפסק הדין [MASK] לביצוע ההריסה [MASK] זאת להב ##די ##ל ממ ##קרים של פסקי דין שהת [MASK] ##ו לבני ##יה על קרקע פרטית , נושא העומד [MASK] סדר העדיפויות בא ##כי ##פה של בנייה בלתי חוקית [MASK] [MASK] \" ש ) . מימוש צווי [MASK] נתון לש ##יקול [MASK] המשיבים , ואין לקרוא [MASK] הדין יותר ממה שיש בו . [MASK] [MASK] נדחתה בפסק הדין החלוט בהע ##דר עילה להתערבות [SEP] הפלמ \" ח 23 בני - ברק , 51 ##11 ##8 ישראל , טלפון : 03 - 61 ##47 ##61 ##4 פקס : 03 [MASK] 57 ##0 ##60 ##44 , דוא \" ל : in ##f [MASK] @ light ##h ##o ##use [MASK] g ##all ##er ##y . co . il [SEP]\nI0421 11:32:54.965553 140538879149440 create_pretraining_data.py:161] input_ids: 2 3953 1437 3 1863 1365 922 3 3293 1331 3 1111 2727 646 361 891 6182 627 3263 801 1773 3 352 3956 634 639 3417 3585 16 2819 6412 3 3403 4793 698 784 751 627 2439 1463 2800 3 3 6 133 13 18 2608 1293 3 3291 728 2407 3 1576 16 1903 2188 3 922 1075 4216 1552 1025 18 3 3 2633 1365 922 2324 3540 798 1945 7264 4 3222 6 115 2829 875 17 2175 16 2830 3853 404 976 16 3323 30 1043 17 1790 2718 5103 389 4250 30 1043 3 4151 387 4353 1815 16 5921 6 120 30 2103 412 3 36 5865 385 386 7234 3 48 4045 928 391 18 1083 18 1681 4\nI0421 11:32:54.965756 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.965951 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0421 11:32:54.966092 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 3 7 10 20 21 29 31 41 42 49 53 58 65 66 85 100 112 117 118 0\nI0421 11:32:54.966243 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 648 2352 12 1773 3246 2819 1793 2009 352 1331 2659 1365 1289 7198 3853 17 386 7234 17 0\nI0421 11:32:54.966395 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0421 11:32:54.966521 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\nI0421 11:32:54.967056 140538879149440 create_pretraining_data.py:149] *** Example ***\nI0421 11:32:54.967331 140538879149440 create_pretraining_data.py:151] tokens: [CLS] כתבי יד על פי הז ##מנה ונ ##דב ##ת ידם של אלה אשר להם הוא הקדיש שירים מיוחדים , ות ##מיכה [MASK] ##ת ##נת [MASK] [MASK] ##י בית המד [MASK] , הוא ניסה להתפר [MASK] ואילו כח ##ז ##ן בבית הכנסת , אולי באותו בית [MASK] הנושא את שמו עד עצם היום הזה [MASK] וב ##זכ ##ות זאת הוא [MASK] תרו ##מות מן [SEP] 合 征 的 主 [MASK] 原 因 。 ” 马 静 [MASK] 示 ， 假 [MASK] 里 ， 相 当 一 部 האו 家 长 因 为 工 作 繁 忙 [MASK] 疏 于 对 [MASK] 子 [MASK] 管 理 ， 而 一 [MASK] 自 控 能 力 不 强 的 孩 子 ， 因 为 ירושלים 闲 的 时 [MASK] 多 [SEP]\nI0421 11:32:54.967566 140538879149440 create_pretraining_data.py:161] input_ids: 2 4604 1005 639 823 930 3137 1058 3434 355 5972 627 1230 910 1888 737 5312 2442 2639 16 1187 3950 3 355 674 3 3 359 766 1965 3 16 737 7094 5502 3 6603 1798 358 360 1229 2426 16 2335 2951 766 3 2445 637 2131 918 6924 1543 1249 3 754 2031 621 1111 737 3 2512 792 1059 4 244 269 302 222 3 241 251 216 207 345 342 3 306 348 234 3 337 348 303 267 217 335 4406 259 339 251 221 263 232 312 271 3 299 227 261 3 255 3 309 293 348 315 217 3 320 275 318 238 218 266 302 257 255 348 251 221 2642 340 302 278 3 253 4\n","name":"stdout"},{"output_type":"stream","text":"I0421 11:32:54.967774 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.967960 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.968159 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 22 25 26 30 35 36 46 54 60 69 76 80 87 96 100 102 108 121 125 0\r\nI0421 11:32:54.968347 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 1635 744 3876 2036 788 743 2426 16 3037 328 327 285 237 348 257 302 228 307 341 0\r\nI0421 11:32:54.968532 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\nI0421 11:32:54.968668 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\r\nI0421 11:32:54.969307 140538879149440 create_pretraining_data.py:149] *** Example ***\r\nI0421 11:32:54.969663 140538879149440 create_pretraining_data.py:151] tokens: [CLS] ##מע ##ט כל אחד מאיתנו מקבל מפע ##ם לפע ##ם הודעת פרסומת [MASK] ##מס או במי ##יל , אפילו שלא ביקש ##נו וכ ##נר ##אה [MASK] [MASK] . החוק גם , [MASK] ##ודד אכי [MASK] [MASK] החלופי ##בעת ממתן תגמ ##ול ללא הוכ ##חת נזק לתו ##בע [MASK] ##כה בדין , ובכך למעשה [MASK] ##סכ ##ות על ##ויות הרש ##ויות בא ##כי ##פה מנהל ##ית [SEP] מיום מעצר ##ו [MASK] [MASK] ##ך 34 ימים בהם שה ##ה איגור במע ##צר לצו ##רכי חקירה , המשטרה לא [MASK] ##חה בפניו שום ראיה הקו ##שרת אותו לביצוע העבירה , ולא נת ##נה לו שום אפשרות להז ##ים את החש ##ד , כך שע ##ד היום איגור אינו יוד ##ע איך או כיצד [MASK] ממה [MASK] [MASK] ##חה . [SEP]\r\nI0421 11:32:54.969942 140538879149440 create_pretraining_data.py:161] input_ids: 2 1104 377 694 965 2756 6008 4008 371 2589 371 4058 4096 3 1213 734 4400 643 16 2595 938 4720 629 924 2874 854 3 3 18 1617 743 16 3 4375 4834 3 3 6997 2375 7458 1534 631 1132 1930 842 1688 3761 980 3 811 5476 16 5377 3030 3 1756 621 639 3810 2940 3810 698 784 751 7292 625 4 1637 3937 352 3 3 394 3055 2491 1577 683 362 1481 2156 1269 4001 2757 3077 16 3215 648 3 831 7172 2509 7095 1739 1658 1016 3293 3315 16 1037 1234 638 970 2509 1728 5138 622 637 2942 353 16 927 902 353 1543 1481 1197 1797 354 3913 734 6483 3 4216 3 3 831 18 4\r\nI0421 11:32:54.970202 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.970423 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.970589 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 12 13 26 27 32 35 36 37 41 48 54 70 71 76 87 114 121 123 124 0\r\nI0421 11:32:54.970746 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 4096 802 2607 1348 724 751 6883 985 631 6108 1515 3962 375 683 4873 1481 734 6658 5112 0\r\nI0421 11:32:54.970925 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\nI0421 11:32:54.971107 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\r\nI0421 11:32:54.971804 140538879149440 create_pretraining_data.py:149] *** Example ***\r\nI0421 11:32:54.972148 140538879149440 create_pretraining_data.py:151] tokens: [CLS] אשר הוש ##פע ##ו עמו ##קות מפור ##ע ##נויות הגל ##ות ומה ##לה ##ט המש ##יח ##י יפ קבלת האר \" י , כמו אח ##יהם בקהילות רבות , קיבלו בהת ##לה ##בות רבה [MASK] היו [MASK] המש [MASK] ##יות של שבתאי צבי ( 16 ##2 בוט – 16 ##76 ) . [MASK] ##שב ##ע [MASK] ##כנ ##זי , הרב השליח מי ##רו ##שלים , מצא במרוקו [MASK] פור ##ייה להפ [MASK] [MASK] ##וי [SEP] 21 קט ##עים מוזיקלי ##ים להם אחראי הכ ##ורי ##אוג ##רף הגדול של האח ##ים ו [MASK] ##נר מש ##נות [MASK] מהרכב [MASK] , בא ##זב ##י ברק ##לי ( רחוב 4 ##2 [MASK] . ברק [MASK] [MASK] האב ##א של הז ' אנ ##ר שה [MASK] . . . המשך [SEP]\r\nI0421 11:32:54.972439 140538879149440 create_pretraining_data.py:161] input_ids: 2 910 3363 1050 352 4240 1189 1369 354 3489 5342 621 2116 673 377 705 1384 359 2490 4690 3501 6 117 16 1155 711 942 4775 1693 16 5643 1430 673 871 2236 3 1017 3 705 3 678 627 4728 4253 12 1536 393 4843 203 1536 4354 13 18 3 889 354 3 3845 1558 16 1196 5675 877 636 2310 16 1861 2157 3 1495 1255 2152 3 3 695 4 3337 1252 952 7193 622 1888 6991 745 1342 6209 4321 1774 627 1241 622 113 3 2874 652 660 3 6542 3 16 698 3447 359 2175 650 12 4037 24 393 3 18 2175 3 3 1971 374 627 930 11 950 372 683 3 18 18 18 1505 4\r\nI0421 11:32:54.972678 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.972931 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.973113 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 18 35 36 37 39 47 53 56 68 72 73 92 96 97 98 109 112 113 122 0\r\nI0421 11:32:54.973269 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 627 637 1017 5461 1384 398 1243 1486 3417 1158 1805 623 112 17 1590 13 650 16 6288 0\r\nI0421 11:32:54.973434 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\nI0421 11:32:54.973586 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\r\nI0421 11:32:54.974216 140538879149440 create_pretraining_data.py:149] *** Example ***\r\nI0421 11:32:54.974574 140538879149440 create_pretraining_data.py:151] tokens: [CLS] במאי 1465 פר ##ץ מרד כנגד [MASK] , וע ##בד אל ח ##א ##ק , הסולטן האחרון מש ##ושלת בני מר ##ין 14 ##20 – 1465 נר [MASK] . [MASK] המר ##ד התח ##ולל [MASK] פר ##עות עק ##ובות מדם יהודי ביה [MASK] [MASK] ובי ##ה ##ודי הער ##ים הש ##כנות , ומי שלא [MASK] ##מל ##ט נאלץ לה ##מיר את דת ##ו או נט ##בח ללא רח הדלת . [MASK] [MASK] [MASK] 14 ##71 – 150 ##5 , [MASK] [MASK] ##רים לש ##וב ליהדות [MASK] . [SEP] זאת [MASK] . העתירה העיקרית לוק ##ה [MASK] פניה בש ##ני פג ##מים , שכל אחד מהם מצדיק , כשלעצמו , דחיית העתירה העיקרית על הסף . [SEP]\r\nI0421 11:32:54.974879 140538879149440 create_pretraining_data.py:161] input_ids: 2 6372 7111 821 392 2447 2091 3 16 795 1041 800 115 374 367 16 4501 3024 652 6575 875 1086 632 1535 6259 203 7111 6011 3 18 3 6295 353 1769 2910 3 821 755 1864 1181 5997 1910 3350 3 3 1188 362 1272 6408 622 656 3152 16 5347 938 3 1605 377 7326 641 6144 637 4862 352 734 2020 1387 1132 1061 4507 18 3 3 3 1535 5105 203 7124 401 16 3 3 681 728 645 7388 3 18 4 1111 3 18 1289 3709 3088 362 3 6059 708 628 2686 775 16 3424 965 2290 6735 16 7486 16 3358 1289 3709 639 2766 18 4 0 0 0 0 0 0 0 0 0 0 0\r\nI0421 11:32:54.975143 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\r\nI0421 11:32:54.975410 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\r\nI0421 11:32:54.975570 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 7 28 30 35 38 43 44 55 65 69 71 72 73 80 81 86 90 96 0 0\r\nI0421 11:32:54.975729 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 7559 2708 2088 352 1864 1272 2357 2726 2020 371 978 4501 4723 1139 3751 371 1242 639 0 0\r\nI0421 11:32:54.975931 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0\r\nI0421 11:32:54.976062 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\r\nI0421 11:32:54.976662 140538879149440 create_pretraining_data.py:149] *** Example ***\r\nI0421 11:32:54.976892 140538879149440 create_pretraining_data.py:151] tokens: [CLS] בש ##דרה החמישי ##ת בניו יורק , חלו ##ן [MASK] ##ווה עוצ ##ב , [MASK] ערב , כס ##ט [MASK] [MASK] [MASK] המצ ##יגות דמויות ניו יורק ##יות שונות [MASK] [MASK] החיים בעיר . לדוגמה , ערב \" “ d [MASK] ##r ##s be ##t ##we ##en fr ##ie ##n [MASK] ##s ” , \" cl ##ub night \" , \" [MASK] ##ort [SEP] ##יו והב ##יא שני עדים המע ##ידים על נכונות ##ן . וכך מתברר [MASK] בן חסין היה מעין [MASK] ##ם ומת ##וו ##ך לעת מצוא [MASK] דבר [MASK] ##וש ##ף פ ##ן מעשי וב ##לתי צפו ##י באי ##שיות ##ו [MASK] הפייטן [MASK] [MASK] מלא ##כות רבות שלא היה בהן כדי [MASK] את ##ולמו . מן הפיוט האישי שכתב על קב ##רו [SEP]\r\nI0421 11:32:54.977105 140538879149440 create_pretraining_data.py:161] input_ids: 2 708 3438 7507 355 4594 3327 16 1232 360 3 1200 3994 369 16 3 2128 16 3082 377 3 3 3 1388 6959 4147 3395 3327 678 2133 3 3 2284 2282 18 3707 16 2128 6 206 45 3 380 384 3723 365 6274 974 2102 1609 382 3 384 207 16 6 5857 1963 5871 6 16 6 3 3636 4 633 3197 779 1062 6044 1343 2162 639 3402 360 18 3003 6665 3 794 899 767 3223 3 371 2171 878 394 1746 6732 3 1130 3 714 376 128 360 6611 754 2032 6064 359 2009 5077 352 3 3296 3 3 1687 884 1693 938 767 5314 954 3 637 4376 18 1059 2420 6395 2027 639 1641 636 4\r\nI0421 11:32:54.977302 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.977519 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.977676 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 10 15 20 21 22 30 31 41 51 62 78 83 90 92 105 107 108 116 118 0\r\nI0421 11:32:54.977827 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 1457 1748 4966 657 2709 1579 3618 3271 396 3728 6107 2489 16 680 627 16 1714 3638 6788 0\r\nI0421 11:32:54.977981 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\nI0421 11:32:54.978105 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\r\nI0421 11:32:54.978664 140538879149440 create_pretraining_data.py:149] *** Example ***\r\nI0421 11:32:54.978884 140538879149440 create_pretraining_data.py:151] tokens: [CLS] אם גודל ##ה של השמש היה [MASK] ##גוד ##ל האות \" ו \" במק ##ל ##דת המחשב שלך [MASK] הכוכב הקרוב ביותר לשמש [MASK] ממו ##ק ##ם כ [MASK] 14 ק \" מ ממ ##ס ##ך [MASK] שלך . להרחבה [MASK] מרחק ##ים וגד ##לים ביקום – מסע במערכת השמש , סדרי [MASK] [MASK] [SEP] הז [MASK] [MASK] ##נטים ליד הרכב . לפי הנסיבות [MASK] סביר שג ##ונ ##ב הרכב הצליח לח [MASK] לת ##א הנהג בעוד התובע עומד עם בעלת הח [MASK] , ממש ליד הדלת האח ##ורית של הרכב כאשר היא בד ##קה דוגמאות של מוצ ##רים . בנסיבות אלו אינני מוצא ##ת ' רשלנות רבתי ' מצד בעל הרכב באשר אדם סביר לא [MASK] היה לחשוש שמא יחד ##ור גנב לרכב באור יום [SEP]\r\nI0421 11:32:54.979097 140538879149440 create_pretraining_data.py:161] input_ids: 2 862 3068 362 627 1361 767 3 2244 361 6388 6 113 6 1171 361 1103 1994 3485 3 2628 4486 1109 3941 3 4934 367 371 119 3 1535 131 6 122 891 368 394 3 3485 18 3705 3 4042 622 4190 882 5412 203 2167 4726 1361 16 4109 3 3 4 930 3 3 4284 2492 998 18 1101 5797 3 2087 1644 1088 369 998 5364 744 3 912 374 2112 2165 2926 3106 765 2941 680 3 16 1076 2492 4507 1241 1964 627 998 983 857 886 780 7347 627 3094 681 18 2477 1209 7372 3393 355 11 1319 1845 11 2307 1714 998 2579 1352 2087 648 3 767 5777 3975 2344 623 4859 2493 3529 1337 4\r\nI0421 11:32:54.979305 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.979528 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.979803 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 7 19 24 29 37 41 53 54 57 58 63 65 69 73 76 83 112 117 126 0\r\nI0421 11:32:54.980046 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 119 17 767 17 1994 30 3068 3252 2522 6481 1101 16 369 1134 2112 660 998 1950 1337 0\r\nI0421 11:32:54.980215 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\nI0421 11:32:54.980428 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\r\nI0421 11:32:54.981056 140538879149440 create_pretraining_data.py:149] *** Example ***\r\nI0421 11:32:54.981294 140538879149440 create_pretraining_data.py:151] tokens: [CLS] אלי ##כם . [MASK] כי מי בז [MASK] [MASK] קטנות , וש ##מח ##ו [MASK] ##ו את [MASK] [MASK] ##ן הב ##די [MASK] ביד זר ##ובב ##ל , שבע ##ה - אלה ; עיני יהוה , המה משוט ##טים בכל - הארץ . יא וא ##ען , ואומר אליו : מה - שני הזיתים ומח , על - ימי ##ן המנו ##רה ועל - שמא ##לה . יב וא ##ען [MASK] , ואומר 30 : מה - שתי שיב ##לי הזיתים , אשר ביד שני צ ##נת ##רות הזהב [MASK] המר ##יקים מעל ##יהם , הזהב [MASK] [SEP] דבר [MASK] ##ווה סכנה בס ##ביבת ילדים . [MASK] ##רון נוסף [MASK] כשה ##מע ##מד הופ [MASK] את המסך לנ ##יי ##ח בצורה מוחלטת על פני שולחן העבודה [SEP]\r\nI0421 11:32:54.981539 140538879149440 create_pretraining_data.py:161] input_ids: 2 1243 1170 18 3 677 877 1186 3 3 7024 16 1010 1696 352 3 352 637 3 3 360 758 646 3 2474 4890 2913 361 16 2431 362 17 1230 31 7278 6790 16 2146 6400 847 906 17 1360 18 2848 722 5025 16 5330 3248 30 702 17 1062 4615 3947 16 639 17 2119 360 5112 710 1366 17 3975 673 18 3757 722 5025 3 16 5330 1590 30 702 17 1869 2869 650 4615 16 910 2474 1062 130 674 904 3642 3 6295 3512 2296 942 16 3642 3 4 1130 3 1200 4722 802 3491 1509 18 3 890 2195 3 1745 1104 888 2012 3 637 2086 1374 793 378 1913 3680 639 1379 5006 3316 4\r\nI0421 11:32:54.981751 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.981948 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.982092 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 4 8 9 15 18 19 21 23 52 55 71 74 90 97 100 107 110 115 119 0\r\nI0421 11:32:54.982247 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 117 16 3389 4884 17 1971 758 361 17 6390 1517 3248 16 18 2146 863 17 394 793 0\r\nI0421 11:32:54.982425 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\nI0421 11:32:54.982627 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\r\nI0421 11:32:54.983202 140538879149440 create_pretraining_data.py:149] *** Example ***\r\nI0421 11:32:54.983429 140538879149440 create_pretraining_data.py:151] tokens: [CLS] הבע ##יה היא , שמה שאני ##ברא משם זה פע ##נו ##ח של \" [MASK] \" פ \" , ולא [MASK] [MASK] ##ו , ולא השלי ##מו . אך בי ##נתיים מצאתי כבר שזה ר ' שמואל פ [MASK] ##יים בספר \" ירי [MASK] שלמה \" . [SEP] רבות של הנאה . אחד [MASK] ##פקיד [MASK] הרבים הינו [MASK] ##ר ברחבי האינטרנט במ ##טרה למצוא משחקים איכות ##יים [MASK] בשב ##יל ##כם וכולם בחינם ! [SEP]\r\nI0421 11:32:54.983666 140538879149440 create_pretraining_data.py:161] input_ids: 2 3235 634 857 16 4999 2779 6344 5172 735 1046 629 378 627 6 3 6 128 6 16 1037 3 3 352 16 1037 1891 664 18 1030 783 5225 5672 1131 2510 132 11 2979 128 3 781 2179 6 2849 3 1656 6 18 4 1693 627 4535 18 965 3 1583 3 5226 1393 3 372 3683 2634 647 1477 2331 1677 2759 781 3 5289 643 1170 6864 3047 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nI0421 11:32:54.983896 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nI0421 11:32:54.984111 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nI0421 11:32:54.984251 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 7 15 16 21 22 37 39 44 54 56 59 69 0 0 0 0 0 0 0 0\r\nI0421 11:32:54.984413 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 1950 1691 6 1046 3429 2979 6203 755 752 629 3761 1390 0 0 0 0 0 0 0 0\r\nI0421 11:32:54.984569 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\r\nI0421 11:32:54.984710 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\r\nI0421 11:32:54.985342 140538879149440 create_pretraining_data.py:149] *** Example ***\r\nI0421 11:32:54.985632 140538879149440 create_pretraining_data.py:151] tokens: [CLS] [MASK] \" ש ( [MASK] [MASK] סי ' א [MASK] תמ ##ה על דברי הרי רשלנותו ף , מדוע שינה מדברי הג ##מ ' [MASK] ##יאה [MASK] הפסוק : \" [MASK] בשדה אחר \" לפטור ש ##ן ורגל ברה \" ר , וכתב [MASK] אחר . [SEP] ובי ##ת הספר מיועד לק ##ליט ##ת כ - 150 תלמידים . המשיבים הצהירו [MASK] , כי [MASK] בית תום יח ##ובר למי ##ם ולח ##ש [MASK] מיד עם העתקת בית הספר שבמ ##תחם , ובה ##וד [MASK] המש ##לי ##מה עד ##כנו כי הכוונה לחבר את בית הספר למי [MASK] באמצעות מיכל [MASK] מים עד ליום 9 . 8 . 2018 , ולאחר ##ונה הצהירו ( בתאריך [MASK] . 8 . 2018 ) כי [MASK] ההריסה לא י [SEP]\r\nI0421 11:32:54.985854 140538879149440 create_pretraining_data.py:161] input_ids: 2 3 6 133 12 3 3 1408 11 108 3 2361 362 639 1918 1120 5600 127 16 3281 4612 6927 730 356 11 3 3984 3 5300 30 6 3 6561 1184 6 2854 133 360 4888 2476 6 132 16 4887 3 1184 18 4 1188 355 1015 7119 901 2272 355 119 17 7124 3676 18 1576 7411 3 16 677 3 766 4278 2668 3809 5214 371 3574 375 3 1636 765 7098 766 1015 6719 1198 16 1934 630 3 705 650 691 918 2255 677 5822 4515 637 766 1015 5214 3 1510 6824 3 2495 918 3389 29 18 28 18 1292 16 5530 919 7411 12 7573 3 18 28 18 1292 13 677 3 1331 648 117 4\r\nI0421 11:32:54.986059 140538879149440 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.986272 140538879149440 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\nI0421 11:32:54.986440 140538879149440 create_pretraining_data.py:161] masked_lm_positions: 1 5 6 10 16 19 25 27 31 44 62 65 67 74 85 98 101 116 123 0\r\nI0421 11:32:54.986609 140538879149440 create_pretraining_data.py:161] masked_lm_ids: 1457 3584 108 13 6 3281 5252 637 6673 2626 7051 4942 1015 1605 4513 371 678 27 1293 0\r\nI0421 11:32:54.986776 140538879149440 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\nI0421 11:32:54.986916 140538879149440 create_pretraining_data.py:161] next_sentence_labels: 1\r\n","name":"stdout"},{"output_type":"stream","text":"I0421 11:32:56.411960 140538879149440 create_pretraining_data.py:166] Wrote 5000 total instances\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Run pretraining data"},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_PARAM='/kaggle/input/pretrained-bert-including-scripts/cased_L-12_H-768_A-12/cased_L-12_H-768_A-12'","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python run_pretraining.py \\\n  --input_file=/kaggle/working/tf_examples.tfrecord \\\n  --output_dir=/kaggle/working/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_PARAM/bert_config.json \\\n  --init_checkpoint=$BERT_PARAM/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=128 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=30 \\\n  --num_warmup_steps=10 \\\n  --learning_rate=2e-5","execution_count":19,"outputs":[{"output_type":"stream","text":"W0421 11:40:47.475868 140390511695232 module_wrapper.py:139] From run_pretraining.py:407: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n\nW0421 11:40:47.476208 140390511695232 module_wrapper.py:139] From run_pretraining.py:407: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n\nW0421 11:40:47.476577 140390511695232 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n\nW0421 11:40:47.478442 140390511695232 module_wrapper.py:139] From run_pretraining.py:414: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n\nW0421 11:40:47.478711 140390511695232 module_wrapper.py:139] From run_pretraining.py:418: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n\nW0421 11:40:47.480068 140390511695232 module_wrapper.py:139] From run_pretraining.py:420: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n\nI0421 11:40:47.480229 140390511695232 run_pretraining.py:420] *** Input Files ***\nI0421 11:40:47.480365 140390511695232 run_pretraining.py:422]   /kaggle/working/tf_examples.tfrecord\nW0421 11:40:47.480541 140390511695232 lazy_loader.py:50] \nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nW0421 11:40:49.804557 140390511695232 estimator.py:1994] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7faedd31b378>) includes params argument, but params are not passed to Estimator.\nI0421 11:40:49.806255 140390511695232 estimator.py:212] Using config: {'_model_dir': '/kaggle/working/pretraining_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faeaed9a0f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\nI0421 11:40:49.807285 140390511695232 tpu_context.py:220] _TPUContext: eval_on_tpu True\nW0421 11:40:49.808060 140390511695232 tpu_context.py:222] eval_on_tpu ignored because use_tpu is False.\nI0421 11:40:49.808216 140390511695232 run_pretraining.py:459] ***** Running training *****\nI0421 11:40:49.808371 140390511695232 run_pretraining.py:460]   Batch size = 32\nW0421 11:40:49.818925 140390511695232 deprecation.py:506] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nW0421 11:40:49.819525 140390511695232 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\nW0421 11:40:49.833842 140390511695232 module_wrapper.py:139] From run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n\nW0421 11:40:49.840306 140390511695232 deprecation.py:323] From run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.experimental.parallel_interleave(...)`.\nW0421 11:40:49.840528 140390511695232 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\nW0421 11:40:49.872024 140390511695232 deprecation.py:323] From run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.experimental.map_and_batch(...)`.\nW0421 11:40:49.872222 140390511695232 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\nW0421 11:40:49.965010 140390511695232 module_wrapper.py:139] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n\nW0421 11:40:50.136067 140390511695232 deprecation.py:323] From run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nI0421 11:40:50.158409 140390511695232 estimator.py:1148] Calling model_fn.\nI0421 11:40:50.158693 140390511695232 tpu_estimator.py:3124] Running train on CPU\nI0421 11:40:50.159272 140390511695232 run_pretraining.py:117] *** Features ***\nI0421 11:40:50.159515 140390511695232 run_pretraining.py:119]   name = input_ids, shape = (32, 128)\nI0421 11:40:50.159686 140390511695232 run_pretraining.py:119]   name = input_mask, shape = (32, 128)\nI0421 11:40:50.159889 140390511695232 run_pretraining.py:119]   name = masked_lm_ids, shape = (32, 20)\nI0421 11:40:50.160066 140390511695232 run_pretraining.py:119]   name = masked_lm_positions, shape = (32, 20)\nI0421 11:40:50.160269 140390511695232 run_pretraining.py:119]   name = masked_lm_weights, shape = (32, 20)\nI0421 11:40:50.160476 140390511695232 run_pretraining.py:119]   name = next_sentence_labels, shape = (32, 1)\nI0421 11:40:50.160639 140390511695232 run_pretraining.py:119]   name = segment_ids, shape = (32, 128)\nW0421 11:40:50.160982 140390511695232 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n\nW0421 11:40:50.162888 140390511695232 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nW0421 11:40:50.188405 140390511695232 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n\n","name":"stdout"},{"output_type":"stream","text":"W0421 11:40:50.223305 140390511695232 deprecation.py:506] From /kaggle/input/bertsrc/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nW0421 11:40:50.239844 140390511695232 deprecation.py:323] From /kaggle/input/bertsrc/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.Dense instead.\nW0421 11:40:50.241720 140390511695232 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nW0421 11:40:52.376452 140390511695232 module_wrapper.py:139] From run_pretraining.py:150: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n\nW0421 11:40:52.381064 140390511695232 module_wrapper.py:139] From run_pretraining.py:165: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n\nI0421 11:40:53.173482 140390511695232 run_pretraining.py:167] **** Trainable Variables ****\nI0421 11:40:53.173785 140390511695232 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.174052 140390511695232 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.174238 140390511695232 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.174460 140390511695232 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.174638 140390511695232 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.174827 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.175058 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.175253 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.175461 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.175631 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.175821 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.175961 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.176126 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.176299 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.176463 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.176603 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.176779 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.176969 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.177145 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.177290 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.177513 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.177709 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.177939 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.178104 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.178249 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.178451 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.178623 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.178770 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.178933 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.179095 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.179239 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.179408 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.179592 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.179791 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.179939 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.180091 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.180242 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.180399 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.180545 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.180699 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.180902 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.181065 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.181225 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.181410 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.181587 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.181734 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.181908 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.182074 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.182207 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.182361 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.182524 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.182668 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.182808 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.182962 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.183119 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.183247 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.183425 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.183576 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.183717 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.183873 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.184033 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.184163 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.184307 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.184473 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.184618 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.184759 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.184916 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.185066 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.185205 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.185372 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.185525 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.185662 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.185825 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.185978 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.186136 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.186285 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.186465 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.186594 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.186734 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.186896 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.187038 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.187179 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.187348 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.187496 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.187623 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.187776 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.187935 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.188060 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.188219 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.188382 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.188531 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.188685 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.188854 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.188994 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.189129 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.189279 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.189456 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.189598 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.189771 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.189923 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.190068 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.190201 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.190369 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.190525 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.190659 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.190810 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.190992 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.191126 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.191286 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.191469 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.191614 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.191783 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.191949 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.192106 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.192263 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.192420 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.192571 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.192716 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.192869 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.193018 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.193182 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.193306 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.193477 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.193625 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.193822 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.193970 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.194126 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.194273 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.194481 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.194658 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.194863 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.195056 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.195210 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.195369 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.195523 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.195678 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.195887 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.196058 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.196196 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.196385 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.196567 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.196747 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.196904 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.197029 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.197196 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.197359 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.197508 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.197645 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.197820 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.197977 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.198126 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.198274 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.198454 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.198581 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.198732 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.198888 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.199028 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.199173 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.199350 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.199531 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.199691 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.199862 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.200036 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.200192 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.200348 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.200511 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.200688 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.200883 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.201037 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.201199 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.201388 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.201547 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.201725 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.201881 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.202027 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.202225 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.202407 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.202576 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.202747 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","name":"stdout"},{"output_type":"stream","text":"I0421 11:40:53.202926 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.203093 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.203244 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.203478 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.203684 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.203891 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.204042 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.204190 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.204365 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.204537 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.204700 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.204894 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.205098 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:40:53.205379 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:40:53.205647 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.205882 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.206075 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.206283 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.206505 140390511695232 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.206701 140390511695232 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.206897 140390511695232 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.207108 140390511695232 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.207355 140390511695232 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.207534 140390511695232 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:40:53.207690 140390511695232 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (28996,), *INIT_FROM_CKPT*\nI0421 11:40:53.207892 140390511695232 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768), *INIT_FROM_CKPT*\nI0421 11:40:53.208081 140390511695232 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,), *INIT_FROM_CKPT*\nW0421 11:40:53.208342 140390511695232 module_wrapper.py:139] From /kaggle/input/bertsrc/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n\nW0421 11:40:53.209591 140390511695232 module_wrapper.py:139] From /kaggle/input/bertsrc/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n\nW0421 11:40:53.423760 140390511695232 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nI0421 11:41:00.629335 140390511695232 estimator.py:1150] Done calling model_fn.\nI0421 11:41:00.631037 140390511695232 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\nI0421 11:41:04.019280 140390511695232 monitored_session.py:240] Graph was finalized.\n2020-04-21 11:41:04.019777: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n2020-04-21 11:41:04.026260: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000160000 Hz\n2020-04-21 11:41:04.026529: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55874d7be570 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-04-21 11:41:04.026636: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2020-04-21 11:41:04.049919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n2020-04-21 11:41:04.470092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2020-04-21 11:41:04.470890: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55874d87ab20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2020-04-21 11:41:04.470929: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n2020-04-21 11:41:04.471203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2020-04-21 11:41:04.471870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\npciBusID: 0000:00:04.0\n2020-04-21 11:41:04.472296: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:41:04.472647: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:41:04.472914: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:41:04.473194: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:41:04.473479: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:41:04.473717: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:41:04.486870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2020-04-21 11:41:04.486909: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n2020-04-21 11:41:04.486961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n2020-04-21 11:41:04.486988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n2020-04-21 11:41:04.487006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \nI0421 11:41:04.488832 140390511695232 saver.py:1284] Restoring parameters from /kaggle/working/pretraining_output/model.ckpt-0\n","name":"stdout"},{"output_type":"stream","text":"W0421 11:41:06.865769 140390511695232 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file utilities to get mtimes.\nI0421 11:41:07.867954 140390511695232 session_manager.py:500] Running local_init_op.\nI0421 11:41:08.139971 140390511695232 session_manager.py:502] Done running local_init_op.\nI0421 11:41:17.430602 140390511695232 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /kaggle/working/pretraining_output/model.ckpt.\nI0421 11:42:39.596090 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0262071\nI0421 11:42:39.597620 140390511695232 tpu_estimator.py:2308] examples/sec: 0.838628\nI0421 11:43:11.979286 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0308802\nI0421 11:43:11.980077 140390511695232 tpu_estimator.py:2308] examples/sec: 0.988166\nI0421 11:43:45.569523 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0297706\nI0421 11:43:45.570004 140390511695232 tpu_estimator.py:2308] examples/sec: 0.95266\nI0421 11:44:17.697156 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0311258\nI0421 11:44:17.697654 140390511695232 tpu_estimator.py:2308] examples/sec: 0.996025\nI0421 11:44:51.386241 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0296833\nI0421 11:44:51.386794 140390511695232 tpu_estimator.py:2308] examples/sec: 0.949864\nI0421 11:45:23.483965 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0311547\nI0421 11:45:23.484584 140390511695232 tpu_estimator.py:2308] examples/sec: 0.996952\nI0421 11:45:56.682009 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0301223\nI0421 11:45:56.682489 140390511695232 tpu_estimator.py:2308] examples/sec: 0.963913\nI0421 11:46:29.247896 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.030707\nI0421 11:46:29.248422 140390511695232 tpu_estimator.py:2308] examples/sec: 0.982624\nI0421 11:47:01.849441 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0306734\nI0421 11:47:01.849981 140390511695232 tpu_estimator.py:2308] examples/sec: 0.981549\nI0421 11:47:35.168052 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0300133\nI0421 11:47:35.168835 140390511695232 tpu_estimator.py:2308] examples/sec: 0.960424\nI0421 11:48:07.222371 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.031197\nI0421 11:48:07.222852 140390511695232 tpu_estimator.py:2308] examples/sec: 0.998304\nI0421 11:48:41.061776 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0295514\nI0421 11:48:41.062577 140390511695232 tpu_estimator.py:2308] examples/sec: 0.945643\nI0421 11:49:13.329996 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0309903\nI0421 11:49:13.330533 140390511695232 tpu_estimator.py:2308] examples/sec: 0.991688\nI0421 11:49:46.605103 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0300525\nI0421 11:49:46.605785 140390511695232 tpu_estimator.py:2308] examples/sec: 0.96168\nI0421 11:50:19.092742 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0307809\nI0421 11:50:19.093198 140390511695232 tpu_estimator.py:2308] examples/sec: 0.984988\nI0421 11:50:52.631759 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0298161\nI0421 11:50:52.632285 140390511695232 tpu_estimator.py:2308] examples/sec: 0.954114\nI0421 11:51:25.028992 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0308668\nI0421 11:51:25.029471 140390511695232 tpu_estimator.py:2308] examples/sec: 0.987737\nI0421 11:51:58.372375 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.029991\nI0421 11:51:58.373051 140390511695232 tpu_estimator.py:2308] examples/sec: 0.959711\nI0421 11:52:31.558210 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0301334\nI0421 11:52:31.559095 140390511695232 tpu_estimator.py:2308] examples/sec: 0.96427\nI0421 11:53:04.340579 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0305042\nI0421 11:53:04.341082 140390511695232 tpu_estimator.py:2308] examples/sec: 0.976134\nI0421 11:53:38.189812 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0295427\nI0421 11:53:38.190336 140390511695232 tpu_estimator.py:2308] examples/sec: 0.945366\nI0421 11:54:10.148209 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0312907\nI0421 11:54:10.148813 140390511695232 tpu_estimator.py:2308] examples/sec: 1.0013\nI0421 11:54:43.718423 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0297884\nI0421 11:54:43.718907 140390511695232 tpu_estimator.py:2308] examples/sec: 0.953228\nI0421 11:55:15.927352 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0310473\nI0421 11:55:15.927889 140390511695232 tpu_estimator.py:2308] examples/sec: 0.993512\nI0421 11:55:49.138046 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0301108\nI0421 11:55:49.138547 140390511695232 tpu_estimator.py:2308] examples/sec: 0.963545\nI0421 11:56:21.057358 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.031329\nI0421 11:56:21.057959 140390511695232 tpu_estimator.py:2308] examples/sec: 1.00253\nI0421 11:56:54.504228 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0298982\nI0421 11:56:54.504787 140390511695232 tpu_estimator.py:2308] examples/sec: 0.956743\nI0421 11:57:27.149436 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0306323\nI0421 11:57:27.149875 140390511695232 tpu_estimator.py:2308] examples/sec: 0.980235\nI0421 11:57:59.566639 140390511695232 tpu_estimator.py:2307] global_step/sec: 0.0308478\nI0421 11:57:59.567115 140390511695232 tpu_estimator.py:2308] examples/sec: 0.98713\nI0421 11:57:59.567781 140390511695232 basic_session_run_hooks.py:606] Saving checkpoints for 30 into /kaggle/working/pretraining_output/model.ckpt.\nI0421 11:58:02.805700 140390511695232 estimator.py:371] Loss for final step: 8.760661.\nI0421 11:58:02.807104 140390511695232 error_handling.py:101] training_loop marked as finished\nI0421 11:58:02.807365 140390511695232 run_pretraining.py:469] ***** Running evaluation *****\nI0421 11:58:02.807549 140390511695232 run_pretraining.py:470]   Batch size = 8\nI0421 11:58:02.856266 140390511695232 estimator.py:1148] Calling model_fn.\nI0421 11:58:02.856570 140390511695232 tpu_estimator.py:3124] Running eval on CPU\nI0421 11:58:02.857139 140390511695232 run_pretraining.py:117] *** Features ***\nI0421 11:58:02.857370 140390511695232 run_pretraining.py:119]   name = input_ids, shape = (8, 128)\nI0421 11:58:02.857573 140390511695232 run_pretraining.py:119]   name = input_mask, shape = (8, 128)\nI0421 11:58:02.857755 140390511695232 run_pretraining.py:119]   name = masked_lm_ids, shape = (8, 20)\nI0421 11:58:02.857903 140390511695232 run_pretraining.py:119]   name = masked_lm_positions, shape = (8, 20)\nI0421 11:58:02.858048 140390511695232 run_pretraining.py:119]   name = masked_lm_weights, shape = (8, 20)\nI0421 11:58:02.858194 140390511695232 run_pretraining.py:119]   name = next_sentence_labels, shape = (8, 1)\nI0421 11:58:02.858375 140390511695232 run_pretraining.py:119]   name = segment_ids, shape = (8, 128)\nI0421 11:58:05.608923 140390511695232 run_pretraining.py:167] **** Trainable Variables ****\nI0421 11:58:05.609255 140390511695232 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.609539 140390511695232 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.609745 140390511695232 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.609921 140390511695232 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.610096 140390511695232 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.610280 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.610473 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.610627 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.610812 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.610996 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.611165 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.611352 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.611522 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.611697 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.611864 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.612049 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:58:05.612216 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:58:05.612426 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.612649 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.612825 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.612998 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.613161 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.613346 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.613515 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.613707 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.613879 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.614047 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.614200 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.614442 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.614619 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.614792 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.614962 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:58:05.615121 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:58:05.615292 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.615489 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.615673 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.615838 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.615996 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.616151 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.616305 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.616524 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.616679 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.616829 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.616985 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.617150 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.617280 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.617470 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.617647 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:58:05.617822 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:58:05.617963 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.618135 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.618302 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.618491 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.618678 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.618840 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.619001 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.619174 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.619342 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.619514 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.619682 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.619862 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.620023 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.620285 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.620476 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:58:05.620671 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:58:05.620837 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.621011 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.621167 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.621329 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.621494 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.621716 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.621895 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.622071 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.622210 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.622387 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.622554 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.622745 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.622909 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.623117 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.623278 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\nI0421 11:58:05.623476 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\nI0421 11:58:05.623659 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.623841 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.623996 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.624153 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.624372 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.624547 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.624787 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.624984 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.625130 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.625302 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.625492 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\nI0421 11:58:05.625692 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.625885 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.626047 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\nI0421 11:58:05.626211 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","name":"stdout"},{"output_type":"stream","text":"I0421 11:58:05.626397 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.626586 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.626762 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.626969 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.627161 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.627375 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.627560 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.627743 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.627923 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.628118 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.628332 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.628540 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.628744 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.628902 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.629268 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.629513 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nI0421 11:58:05.629734 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.629969 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.630271 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.630476 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.630684 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.630877 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.631054 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.631232 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.631437 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.631676 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.631902 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.632055 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.632207 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.632362 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.632507 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.632742 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nI0421 11:58:05.632950 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.633104 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.633295 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.633491 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.633676 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.633865 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.634029 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.634188 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.634379 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.634548 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.634723 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.634863 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.635030 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.635195 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.635369 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.635514 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nI0421 11:58:05.635684 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.635839 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.635976 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.636150 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.636329 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.636488 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.636652 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.636795 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.636957 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.637111 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.637249 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.637412 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.637582 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.637734 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.637861 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.638033 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nI0421 11:58:05.638216 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.638459 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.638641 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.638822 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.638987 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.639175 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.639343 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.639492 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.639683 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.639857 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.640016 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.640165 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.640349 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.640504 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.640677 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.640834 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nI0421 11:58:05.640971 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.641124 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.641309 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.641499 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.641645 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.641792 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.641955 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.642108 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.642259 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.642459 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.642639 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.642803 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.642997 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.643128 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.643278 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.643452 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nI0421 11:58:05.643613 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.643751 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.643930 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.644105 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.644248 140390511695232 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.644475 140390511695232 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.644642 140390511695232 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.644823 140390511695232 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.644999 140390511695232 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.645159 140390511695232 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.645307 140390511695232 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.645489 140390511695232 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (28996,), *INIT_FROM_CKPT*\r\nI0421 11:58:05.645674 140390511695232 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768), *INIT_FROM_CKPT*\r\nI0421 11:58:05.645830 140390511695232 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,), *INIT_FROM_CKPT*\r\nW0421 11:58:05.652211 140390511695232 module_wrapper.py:139] From run_pretraining.py:198: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\r\n\r\nW0421 11:58:05.669553 140390511695232 module_wrapper.py:139] From run_pretraining.py:202: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\r\n\r\n","name":"stdout"},{"output_type":"stream","text":"I0421 11:58:05.720471 140390511695232 estimator.py:1150] Done calling model_fn.\nI0421 11:58:05.743078 140390511695232 evaluation.py:255] Starting evaluation at 2020-04-21T11:58:05Z\nI0421 11:58:06.339923 140390511695232 monitored_session.py:240] Graph was finalized.\n2020-04-21 11:58:06.341219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2020-04-21 11:58:06.341999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\npciBusID: 0000:00:04.0\n2020-04-21 11:58:06.342377: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:58:06.342568: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:58:06.342699: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:58:06.342820: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:58:06.342945: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:58:06.343057: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64\n2020-04-21 11:58:06.343100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2020-04-21 11:58:06.343126: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n2020-04-21 11:58:06.343161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n2020-04-21 11:58:06.343182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n2020-04-21 11:58:06.343200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \nI0421 11:58:06.344372 140390511695232 saver.py:1284] Restoring parameters from /kaggle/working/pretraining_output/model.ckpt-30\nI0421 11:58:07.308632 140390511695232 session_manager.py:500] Running local_init_op.\nI0421 11:58:07.407246 140390511695232 session_manager.py:502] Done running local_init_op.\nI0421 11:58:33.992726 140390511695232 evaluation.py:167] Evaluation [10/100]\nI0421 11:58:58.739925 140390511695232 evaluation.py:167] Evaluation [20/100]\nI0421 11:59:23.138421 140390511695232 evaluation.py:167] Evaluation [30/100]\nI0421 11:59:49.047487 140390511695232 evaluation.py:167] Evaluation [40/100]\nI0421 12:00:13.784518 140390511695232 evaluation.py:167] Evaluation [50/100]\nI0421 12:00:39.742993 140390511695232 evaluation.py:167] Evaluation [60/100]\nI0421 12:01:04.074386 140390511695232 evaluation.py:167] Evaluation [70/100]\nI0421 12:01:29.079286 140390511695232 evaluation.py:167] Evaluation [80/100]\nI0421 12:01:54.000147 140390511695232 evaluation.py:167] Evaluation [90/100]\nI0421 12:02:18.291395 140390511695232 evaluation.py:167] Evaluation [100/100]\nI0421 12:02:18.417459 140390511695232 evaluation.py:275] Finished evaluation at 2020-04-21-12:02:18\nI0421 12:02:18.417829 140390511695232 estimator.py:2049] Saving dict for global step 30: global_step = 30, loss = 8.56319, masked_lm_accuracy = 0.021613833, masked_lm_loss = 8.562686, next_sentence_accuracy = 1.0, next_sentence_loss = 0.0010693793\nI0421 12:02:19.310366 140390511695232 estimator.py:2109] Saving 'checkpoint_path' summary for global step 30: /kaggle/working/pretraining_output/model.ckpt-30\nI0421 12:02:19.311281 140390511695232 error_handling.py:101] evaluation_loop marked as finished\nI0421 12:02:19.311604 140390511695232 run_pretraining.py:483] ***** Eval results *****\nI0421 12:02:19.311773 140390511695232 run_pretraining.py:485]   global_step = 30\nI0421 12:02:19.312045 140390511695232 run_pretraining.py:485]   loss = 8.56319\nI0421 12:02:19.312245 140390511695232 run_pretraining.py:485]   masked_lm_accuracy = 0.021613833\nI0421 12:02:19.312439 140390511695232 run_pretraining.py:485]   masked_lm_loss = 8.562686\nI0421 12:02:19.312590 140390511695232 run_pretraining.py:485]   next_sentence_accuracy = 1.0\nI0421 12:02:19.312762 140390511695232 run_pretraining.py:485]   next_sentence_loss = 0.0010693793\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"global_step = 30  # we need to run it for 10000 epochs\nloss = 8.56319\nmasked_lm_accuracy = 0.021613833\nmasked_lm_loss = 8.562686\nnext_sentence_accuracy = 1.0\nnext_sentence_loss = 0.0010693793"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}